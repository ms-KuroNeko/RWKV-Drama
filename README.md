# **这是啥玩意？**

**【￣^￣】一个基于 [*RWKV_Role_Playing*](https://github.com/shengxia/RWKV_Role_Playing/) 二次开发的小玩具，实现简单的角色扮演和剧本推演，支持多人对话**

⚠️请务必注意，这个系统设计的本意是用来帮助网文写手进行剧情推演，捎带着尝试使用AI参与直播，


# **如果你用过 *RWKV_Role_Playing* 或者了解 *RWKV* 可以跳过本段**

⚠️因为目前RWKV是大语言模型中几乎是最轻量化的系统，所以它的智能水平非常不稳定，**跟现有的助手型AI完全是两码事**

⚠️**本系统基于RWKV模型生成的回答无论是从政治与伦理道德角度还是事实正确的角度都极为不可靠，并不具备在生产环境中脱离人类监管独立运作的可能性，请不要盲目相信模型输出的任何内容**

⚠️**请不要将它用于除了寻找灵感和娱乐之外的任何具体的有风险的可能导致灾难性后果的生产行为，比如无人全自动直播，比如无人全自动直播，比如无人全自动直播**

⚠️**如果你一句人话都不听，出了事别找我，我只负责看乐子**

## 太棒了，是不是我可以有自己的AI娘可以为所欲为了？

艹智障是犯法的，同志，人工智障也是智障【#-_-】

## 那有没有希望她可以更聪明一点？

有，给你三个选项
- 等黄狗良心发现 A100 80G 换不锈钢盆
- 等 某鸽子Bo 和 跑路的黄鹤老板 终于玩够了AI写歌开始做 RWKV-5-World-CHNTurned-28B-ProPlusMaxS+
- 继续用JB疯狂攻击小克【指用JailBreak越狱词忽悠Claude陪你过家家】

## 配置需求

**与大多数基于 RWKV 的AI本地系统要求差不多**

此处以运行一个7B模型的典型配置需求为例

- **24G以上内存**
  - 16G可以运行，但是加载7B模型时会爆内存，需要你用阳寿、运气和虚拟内存帮你顶过去，或者使用预量化模型
- **8-16G显存的显卡**
  - 7B模型以FP16+INT8混合模式运行需要接近8G显存
  - 7B模型以FP16模式运行需要15G显存【可以用两张8G显卡拼凑使用】
  - **最好是N卡9系以后产品**，A卡需要折腾，小白可以直接放弃打出GG
- 不低于 *E3 v2* 或者 *2-3代 i5* 水平的CPU
- 至少能放下完整一个 RWKV 7B 模型的硬盘空间 
  - **一个RWKV 7B模型 一般14G**


## 准备一个 *RWKV* 模型

参考隔壁[模型推荐]()

# **首次使用**

## Python标准环境
跟 *RWKV_Role_Playing* 一样，都是老熟人了：
- Python 3.10
- torch 2.0.1
- tokenizers
- prompt_toolkit
- rwkv
- fastapi
- numpy
- ninja

()[https://docs.python.org/zh-cn/3/library/venv.html]

详见 *关于环境给小白的建议*


### 把本项目直接git clone 到本地或者下载到本地

累了，这步不说了，不会自己Bing解决

### 然后是启动
> python3 server_with_fastapi.py 

由于东西直接扒的 *RWKV_Role_Playing* ，所以高级参数也基本上一样

> python3 server_with_fastapi.py --listen --port=57860 --strategy 'cuda:0 fp16 *11 -> cuda:1 fp16 *11 -> cuda:2 fp16' --model="./../Model/World/xiaol/RWKV-claude-4-World-7B-20230805-ctx65k.pth" --cuda_on=1 --jit_on=1

翻译：
- **--listen**：别他喵的光给我盯着127.0.0.1，把所有网卡都监听上【**非本机访问(比如局域网用手机玩之类的需求)都要记得加这个参数**】
- **--port=57860**：呃……我觉得正常人都不用我解释，如果需要我解释，你也就告别这些东西了……
  - 默认端口号就是57860，其实我多余写这个
-  **--strategy 'cuda:0 fp16 *11 -> cuda:1 fp16 *11 -> cuda:2 fp16'**：把RWKV 7B的33个神经网络层，拆分成三个11层分别放到0 1 2三块支持CUDA的显卡上，运算使用FP16精度
   -  【其实两张8G显存的显卡也是可以完美运行FP16的7B模型的，你按照17+16去拆就行】
   -  【单张8G只能 cuda fp16i8，注意RWKV跟传统的变形金刚(Transformer)不一样，部分量化到int8之后效果极差几乎不能使用，这里推荐有条件尽量双8G跑FP16】
   -  使用CPU跑的话，把“cuda”换成“cpu”就行【很不推荐】
- **--model=喵了个咪咪了个喵.pth**：也是但凡需要讲都侮辱了咱俩智商的东西……
  - *【你不下模型你玩AI，你玩他有啥用啊~】*
- --cuda_on=1：使用CUDA算子编译
  - 注意Linux需要Ninja、PythonDev、GCC+G++等一大堆东西
  - Windows更为痛苦，不仅需要以上全部还需要MSVC++，反正我没折腾明白摆烂了
- --jit_on=1：即时编译机器码
  - 反正具体我也稀里糊涂，总之你知道一个事情就行：**这俩货在我这里可以降低10%左右的显存消耗，提速15%左右，还是很有意义的**
  - **前期为了快速搭建先玩上可以把他俩关掉先跑起来再说**，但是有条件静下心来一定要搞一下
- **推荐的最稳妥的格式是**
  - > python3 server_with_fastapi.py --参数名="参数内容"
  - **注意参数里有空格的一定要用单双引号把字符串套起来，不然会被识别成多个参数**
  - 不然你问我为什么我的strategy怎么写都报错的时候，我一定会大肆嘲笑【喇↑叭↓.mp3】

## **一些重要信息**
### **三个静态前端网址**
- http://localhost:57860/Tester/NewChat.html ：**装填新剧本，提交新的背景故事和人设**
- http://localhost:57860/Tester/Chat.html ：**开始与AI互动**
- http://localhost:57860/Tester/Output.html ：**核心页面的猴版，方便你右键保存成mhtml留念**
- 许愿池啊许愿池，我希望**自己局域网使用或者改了端口号之后跑来问我为什么打不开的傻子**能少一点【喵的一下就哭了】

## **Save文件夹结构**
这里是你保存的剧本~~黑历史~~数据
内部典型结构如下
- 第一层是剧本系列，对应 *ScenarioSeries* 字段
- 然后里面的第二层文件夹是单个剧本，名字对应 *ScenarioName* 字段
- 里面的Log.json是保存的历史对话，如 *20230713-1952_Log.json*
- 里面的 *场景剧本.json* 是自动保存的不带历史对话的裸剧本
- 如果你需要显示头像，请把头像放在对应文件夹下并命名为 *角色名.png*
- **你需要经常操作这个文件夹，我个人建议局域网使用一定要开SMB拉个共享**

## **开始第一次使用**

### 载入剧本
1. 利用我上面讲的关于Save文件夹结构的部分，找到我写好的剧本
2. 打开 *NewChat页面* ，复制粘贴装填剧本
   1. 注意要使用中间这个 *由JSON导入* 
   2. 注意要使用 *场景剧本.json* 里的内容，而不是Log里的
   3. 贴进去之后点 *提交*
   4. 等待片刻后前往 *Chat页面* ，刷新几次到你看见内容为止【这段时间显卡在处理剧本】

### **开始推演剧情**
#### 下方的对话系统
##### 高级

上面是添加一条内容
在前面选择一个人物，在后面写上内容，点击 *发送* 按钮会插入一条内容到剧情里

下面是请求一次生成
在前面选择一个人物，点击后面 *生成* 按钮会触发AI针对该角色产生的一波生成，AI将试图扮演该角色回答你

注意高级面板中，位于中间的文本框是多行的，快速提交的快捷键是[Ctrl]+[Enter]

每次生成会输出3-5条内容，你可以点击 更多 或者 再次点击 生成 来生成更多可能的选项，直到出现你满意的内容为止
把鼠标移到选项右边，可以手动编辑该选项

在中间的文本框里可以指定该角色本条生成内容的前半句
当你在中间填入“愿意”时，AI的回答将会以“我愿意”开头，如“我愿意和你回家”
通过这个机制你可以强制AI做出你想要的回答，避免AI角色不按照你的预期行动
【当然你也可以用上面直接提交功能直接手动以AI扮演的身份插入一条内容来解决这个问题】


##### 快速
使用之前在剧本装填时位于右侧的第一个默认角色发出一条内容，并自动触发左侧第一个默认角色的一条回复
回车可以自动提交【不支持多行】


##### 未决内容
未决内容是没有得到你认可和确认的内容
你可以随时点击 丢弃未决内容 退回你最后的检查点
也可以用 认可并继续推进 把当前的未决内容确认下来
⚠️ 注意，默认保存等操作自动忽略未决内容，请先采纳这部分剧情再保存

##### 保存回放
将当前剧本和对话保存为JSON，目前由于某喵太懒还没做下载功能……


# 进阶内容





## 相关~~抄过的~~项目

-  BlinkDL
   -  [ChatRWKV](https://github.com/BlinkDL/ChatRWKV) 
   -  [RWKV-LM](https://github.com/BlinkDL/RWKV-LM)
- shengxia
  - [RWKV_Role_Playing](https://github.com/shengxia/RWKV_Role_Playing/)
- oobabooga
  - [text-generation-webui](https://github.com/oobabooga/text-generation-webui/)


# 在最后明目张胆的讨个饭

![](/教程图片/赞助.jpg)

我先放两个碗在这里，你们看颜色行事

【我在杭州，蓝的好用一点】